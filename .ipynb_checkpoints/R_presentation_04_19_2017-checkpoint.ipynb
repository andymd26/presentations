{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals of the presentation:\n",
    "1. A brief introduction to R, specifically the dplyr and ggplot2 packages, and Jupyter Notebooks\n",
    "2. Run through an example analysis \n",
    "\n",
    "This presentation is available in my Github at https://github.com/andymd26/presentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About me (and a little soap box speech)\n",
    "1. About me\n",
    "2. What is your background? All mechanical and civil engineers? Software knowledge? Statistics coursework?\n",
    "3. How many of you have a personal webpage? github? Know what github is?\n",
    "4. Soapbox portion: Build a personal website to market yourself. I built mine with square space but many other options exist (www.andrewblohm.com).\n",
    "5. Soapbox cont'd: If you have any interest in doing datascience or computer science work start a github account (https://github.com/andymd26). Usually, one of the first questions you'll get in an interview process if not on the application form itself (great way to demonstrate your coding abilities)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Jupyter notebook?\n",
    "1. Short answer, this. I find it a useful place to sketch out certain types of projects and presentations.\n",
    "2. The Jupyter Notebook is an open-source web application that allows you to create and share documents that contain live code, equations, visualizations and explanatory text. Uses include: data cleaning and transformation, numerical simulation, statistical modeling, machine learning and much more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is R?\n",
    "\"R is an open source programming language and software environment for statistical computing and graphics that is supported by the R Foundation for Statistical Computing. The R language is widely used among statisticians and data miners for developing statistical software and data analysis\" (Wikipedia, 2017)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantages of using R\n",
    "1. R is a programming language and environment developed for statistical analysis by practising statisticians and researchers. \n",
    "2. The graphical capabilities of R are outstanding, providing a fully programmable graphics language that surpasses most other statistical and graphical packages.  \n",
    "3. The validity of the R software is ensured through openly validated and comprehensive governance as documented for the US Food and Drug Administration (R Foundation for Statistical Computing, 2008). Because R is open source, unlike closed source software, it has been reviewed by many internationally renowned statisticians and computational scientists.\n",
    "4. R is free and open source software, allowing anyone to use and, importantly, to modify it. \n",
    "5. R has no license restrictions (other than ensuring our freedom to use it at our own discretion), and so we can run it anywhere and at any time, and even sell it under the conditions of the license.\n",
    "6. Anyone is welcome to provide bug \f",
    "xes, code enhancements, and new packages, and the wealth of quality packages available for R is a testament to this approach to software development and sharing.\n",
    "7. Over 4800 packages available from multiple repositories specializing in topics like econometrics, data mining, spatial analysis, and bio-informatics.\n",
    "8. R is cross-platform.\n",
    "9. R plays well with many other tools, importing data, for example, from CSV \f",
    "les, SAS, and SPSS, or directly from Microsoft Excel, Microsoft Access, Oracle, MySQL, and SQLite. It can also produce graphics output in PDF, JPG, PNG, and SVG formats, and table output for LATEX and HTML.\n",
    "10. R has very active user groups where questions can be asked and are often quickly responded to, often by the very people who developed the environment|this support is second to none (See Stackoverflow).\n",
    "11. New books for R (the Springer Use R! series) are emerging, and there is now a very good library of books for using R.\n",
    "\n",
    "(Source: http://analyticstrainings.com/?p=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "require(\"IRdisplay\")\n",
    "display_png(file=\"stackoverflow.png\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job Announcements by software\n",
    "(Source: Muenchen, 2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display_png(file=\"R_jobs.png\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R vs Python\n",
    "(Source: Muenchen, 2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display_png(file=\"R_Python.png\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trends in Academia Software use (Journal articles)\n",
    "(Source: Muenchen, 2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display_png(file=\"Academia_Software.png\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disadvantages of R\n",
    "1. R has a steep learning curve but no steeper than for other statistical languages.  \n",
    "2. Documentation is sometimes patchy and terse, and impenetrable to the non-statistician. However, some very high-standard books are increasingly plugging the documentation gaps. [Academics has really stepped into the gap]\n",
    "3. The quality of some packages is less than perfect, although if a package is useful to many people, it will quickly evolve into a very robust product through collaborative \u000b",
    "efforts.\n",
    "4. There is, in general, no one to complain to if something doesn’t work. R is a software application that many people freely devote their own time to developing. Problems are usually dealt with quickly on the open mailing lists, and bugs disappear with lightning speed. \n",
    "5. Many R commands give little thought to memory management, and so R can very quickly consume all available memory. This can be a restriction when doing data mining. There are various solutions, including using 64 bit operating systems that can access much more memory than 32 bit ones.\n",
    "\n",
    "(Source: http://analyticstrainings.com/?p=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disk Drive Case Study\n",
    "The first part of any R code is installing and loading the packages that are needed for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "install.packages(\"ggplot2\", repos='http://cran.us.r-project.org')\n",
    "# ggplot is a graphics creation package \n",
    "install.packages(\"dplyr\", repos='https://cran.rstudio.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "require(ggplot2)\n",
    "require(dplyr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are the manager of a disk drive factory producing a small but key part of the assembly. The part should be 4 mm with a tolerance of +/- 0.005mm. Beyond that tolerance the part is unacceptable and would have to be thrown away. You can run the machines that produce this key part at a variable speed (not just fast or slow) parameterized by a key value r for rate expressed in parts/day. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mu= 4\n",
    "rate= 1000\n",
    "sigma= 0.8*(10^-5)*rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a target of producing 1000 parts/day. Clearly, you may not get all 1000 parts since some of them will be rejected if they don't have acceptable widths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vary the rate r from 500 to 1500 parts/day (minimum is 500 and maximum is 1500 for this type of machine). You should vary the number of simulation samples to show why your results are robust and don't depend on the number of sampled values. You need to try a wide range of sampled values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n= 100000\n",
    "data= data.frame(x= rnorm(n= n, mean= mu, sd = sigma))\n",
    "# R has built in normal distribution functions that can be used to generate the Density distribution function, \n",
    "# quantile function or be used to generate random numbers for the normal distribution with mean equal to mean \n",
    "# and standard deviation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ggplot(data= data, aes(x=x)) + geom_density() + labs(x=\"Size (mm)\", y=\"Density\", title= \"Density plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use the above code to walk through several values of the rate. Another approach would be to sample from the available production rates, which are between 500 and 1500 units per day. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization vs. loops\n",
    "From the Wikipedia page on Vectorization, \"In computer science, array programming languages (also known as vector or multidimensional languages) generalize operations on scalars to apply transparently to vectors, \n",
    "matrices, and higher-dimensional arrays\". In these languages, an operation that operates on entire arrays can be called a vectorized operation,[2] regardless of whether it is executed on a vector processor or not.\n",
    "\n",
    "R and Matlab are interpreted languages. The reason loops are slow in an interpreted language is that each function call comes with a bit of an overhead. Thus, when you call a function once with an array of a million elements, you're much faster than calling it a million times with a scalar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k = 10\n",
    "# Number of rate values to try\n",
    "r = round(runif(n= k, min= 500, max= 1500))\n",
    "# Choose 'k' production rates\n",
    "# r = round(runif(n, min= 500, max= 1500))\n",
    "# We could use the code above to explore the entire range of values but it would be overkill here\n",
    "\n",
    "data= data.frame(rate = r[round(runif(n, min= 1, max= length(r)))])\n",
    "# The variable that we need to sample is the rate of production. This code samples from the length of the index \n",
    "# (i.e., number of rates) 'n' times. From this we can generate the additional information that we need.\n",
    "sort(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "There is something wrong with the code above, can anyone identify what the issue is? Specifically, this line:\n",
    "\n",
    "    round(runif(n, min=1, max= length(r)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hist(round(runif(n, min=1, max= length(r))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is going on here? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The short answer is rounding with our bounds creating different probabilities of occurrence in a case when we \n",
    "# want them to be the same. A slight change to the code fixes the problem.\n",
    "\n",
    "data= data.frame(rate = r[round(runif(n, min= 0.5, max= length(r)+0.5))])\n",
    "table(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DPLYR Package\n",
    "The dplyr package is a very useful package in R for data cleaning and exploration:\n",
    "1. It simplifies how you can think about common data manipulation tasks.\n",
    "2. It provides simple “verbs”, functions that correspond to the most common data manipulation tasks, to help you translate those thoughts into code.\n",
    "3. It uses efficient data storage backends, so you spend less time waiting for the computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.1 = data %>%\n",
    "  # Pipe operators, dplyr has a number of verbs (mutate, select, filter, arrange, ...) \n",
    "  mutate(sigma= 0.8*(10^-5)*rate) %>%\n",
    "  # Generate the standard deviation value given the sampled rate\n",
    "  mutate(mu = mu) %>%\n",
    "  # Mean value (same for all entries)\n",
    "  mutate(x = rnorm(n= n, mean= mu, sd= sigma)) %>%\n",
    "  # Sample from the normal distribution using the parameter values we generated (this is a vectorized \n",
    "  # implementation) %>%\n",
    "  group_by(rate) %>%\n",
    "  mutate(p_reject = sum(x>= (mu + 0.005) | x<= (mu - 0.005))/length(x)) %>%\n",
    "  mutate(pass = sum(x <= mu + 0.005 & x >= mu-0.005)) %>%\n",
    "  mutate(pass_perc = pass/n) \n",
    "head(data.1)\n",
    "table(data.1$rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ggplot(data=data.1, aes(x=x)) + geom_density(aes(group=rate, color=rate)) + labs(x=\"Size (mm)\", y=\"Density\", title= \"Density plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ggplot(data=data.1, aes(x=x)) + geom_density() + facet_wrap(~rate) + labs(x=\"Size (mm)\", y=\"Density\", title= \"Density plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ggplot(data= data.1, aes(rate)) + \n",
    "geom_line(aes(y=pass_perc), color= \"blue\") +\n",
    "geom_line(aes(y=p_reject), color= \"red\") + \n",
    "labs(x=\"Production Rate\", y=\"Percentage\", title= \"Acceptance and rejection rates versus production speed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "% Disk drive example\n",
    "clear\n",
    "hold off\n",
    "tic\n",
    "n=120000;\n",
    "u=rand(n,12);\n",
    "mean=4.0;\n",
    "sigma_low=0.0019;\n",
    "sigma_high=0.0026;\n",
    "for i=1:1:n\n",
    "z(i)=sum(u(i,1:12))-6;\n",
    "% use the same z-value for both low and high\n",
    "    low(i)=mean+sigma_low*z(i);\n",
    "    if (low(i)>4.005)      low_cost(i)=31.15;\n",
    "    elseif (low(i)<3.995)  low_cost(i)=31.15;\n",
    "    else                   low_cost(i)=20.75;\n",
    "    end\n",
    "    \n",
    "    high(i)=mean+sigma_high*z(i);\n",
    "    if (high(i)>4.005)     high_cost(i)=30.85;\n",
    "    elseif (high(i)<3.995) high_cost(i)=30.85;\n",
    "    else                   high_cost(i)=20.45;\n",
    "    end\n",
    "end\n",
    "p_low_defective_cost  =sum(low_cost>20.75)/n\n",
    "p_low_ok_cost         =sum(low_cost==20.75)/n\n",
    "exp_cost_low          =p_low_defective_cost*31.15+p_low_ok_cost*20.75\n",
    "\n",
    "p_high_defective_cost =sum(high_cost>20.45)/n\n",
    "p_high_ok_cost        =sum(high_cost==20.45)/n\n",
    "exp_cost_high         =p_high_defective_cost*30.85+p_high_ok_cost*20.45\n",
    "\n",
    "disp(['Low speed:  P(Defective|low speed)=',num2str(p_low_defective_cost)])\n",
    "disp(['Low speed:  P(Not Defective|low speed)=',num2str(p_low_ok_cost)])\n",
    "disp(['High speed: P(Defective|high speed)=',num2str(p_high_defective_cost)])\n",
    "disp(['High speed: P(Not Defective|high speed)=',num2str(p_high_ok_cost)])\n",
    "\n",
    "disp(['Expected cost, Low speed: =$',num2str(exp_cost_low)])\n",
    "disp(['Expected cost, High speed:=$',num2str(exp_cost_high)])\n",
    "% Plotting\n",
    "subplot(2,1,1)\n",
    "hist(low_cost,100)\n",
    "%axis([3.97 4.03 0 2000])\n",
    "title 'low'\n",
    "grid on\n",
    "\n",
    "subplot(2,1,2)\n",
    "hist(high_cost,100)\n",
    "%axis([3.97 4.03 0 2000])\n",
    "title ('high')\n",
    "grid on\n",
    "\n",
    "toc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
